{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section*{RMS Titanic - Predicting survivors in the great maritime disaster}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "This report aims to examine these differences in depth. We inted to create an accurate model for predicting survival probabilities of individuals. We chose survival status as our dependent variable and age, passenger class, sex, and city of embarktion to be our independent variables. We aim at obtaining new insights into the effect these features had on the survival of the participants in this great disaster.\n",
    "\n",
    "# Data\n",
    "\n",
    "The dataset we worked on was ready-made and aquired from Vanderbilt Biostatistics dataset collection (http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.html). The dataset relies on multiple sources major one being the Encyclopedia Titanica webpage. It contains information about all 1313 passengers including their whole names, sex, age, passenger class, and survival status among others. The dataset does not include information about the RMS Titanic crew. Full description of dataset fields is included below.\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Data features}\n",
    "\\input{tex/dtypes.tex}\n",
    "\\end{table}\n",
    "\n",
    "We chose to investigate the effect of sex, age, passenger class, and port of embarktion on the survival probability. In addition, we tested whether being child or an elderly had effect on survival. The motivation behind this was to examine if the assumed policy of \"women and children first\" can be backed up with data. This was done by deriving two additional features, child and elderly, denoting passengers under the age of 15 and over the age of 65, respectively.\n",
    "\n",
    "The dataset contained a designated variable for expressing whether individual had access to lifeboat. This variable was evidently correlated with survival (54\\%). However, we decided to exclude this variable, as we aimed to find the underlying reasons for some people ending in life boats and others not. \n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "The dataset required preprocesing before it could be used in modelling. First, categorical variables (sex, passenger class, city of embarktion) were transformed into binary representation. Second, all rows including missing values were dropped out. It is notable that this removes major part of the original data. Moreover, has different effect on different passenger classes as over 70% of 3rd class passenger data is lost. Third, age variable was scaled and normalized to avoid its larger magnitude to skew results. Finally, data set was divided into training (67 %) and test (33 %) set.\n",
    "\n",
    "## Key Statistics\n",
    "\n",
    "Passenger class specific key statistics were computed for the dataset and shown in the table below.\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Key statistics by passenger class}\n",
    "\\resizebox{0.95\\textwidth}{!}{\n",
    "    \\input{tex/key_stats.tex}\n",
    "}\n",
    "\\end{table}\n",
    "\n",
    "The likelihoods of survival differ between different passenger classes. 60\\% of the first class passengers survived, as compared to 19\\% of the third class passengers. 711 of the passengers were travelling in the third class which is more than first and second class passengers combined. However, most of the third class information is missing which might introduce bias to the model. First class passengers were evidently older than second and third class passengers, which might increase the correlation between age and surivaval probability. \n",
    "\n",
    "In addition, we analyzed the correlation between explanatory variables. The correlations are shown in the heatmap and table below. Besides the evident correlations (between passenger classes, between port of embarktion, one being either male or female, age and the derived quantities), there seems to be not many noteworthy correlations. One worth mentioning is the correlation beween age and first passenger class (0.43)\n",
    "\n",
    "![Heatmap of correlations between explanatory variables](figs/corr_matrix.png)\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Correlations between explanatory variables}\n",
    "\\resizebox{0.95\\textwidth}{!}{\n",
    "    \\input{tex/corr_matrix.tex}\n",
    "}\n",
    "\\end{table}\n",
    "\n",
    "\n",
    "# Model\n",
    "\n",
    "Our choice is to examine the data with logistic regression. In logistic regression, linear regression is nonlinearized with a logistic function. The function limits regression values between 0 and 1 and thus provides a prozy for probability of survival.\n",
    "\n",
    "## Prior choice\n",
    "\n",
    "As we don't have prior knowledge on how the data is distributed, a weakly informative prior is chosen according to stan development team guidelines. The rationale for using weakly informative prior is to let inferences be unaffected by information external to the current data - \"let the data speak for itself\", so to say. A Student-t distribution with zero mean is a suitable choice for logistic regression (see i.e. https://arxiv.org/pdf/0901.4011.pdf for more information). \n",
    "\n",
    "Thus, for each regression coefficient, we assume a Student-t prior distribution with mean 0, degrees-of-freedom parameter $\\nu$ within the range $3 < \\nu < 7$, and scale $s$, where $s$ is chosen to provide weak information on the expected scale. The figure below presents a few prior distribution candidates like this.\n",
    "\n",
    "![Student-t distributions with different scales and degrees of freedom](figs/prior_choice.png)\n",
    "\n",
    "## Model Selection\n",
    "\n",
    "This prior is further adjusted in model selection. Several different combinations of degrees of freedom and scale values for prior were compared by applying PSIS-LOO cross validation to the logistic model. The table and the figure below present results of the cross validation. It can be seen that prior with 6 degrees of freedom and scale as 1 provides the best fit based on the PSIS-LOO value and corresponding k values.\n",
    "\n",
    "\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{PSIS-LOO cross validation results}\n",
    "\\resizebox{0.45\\textwidth}{!}{\n",
    "    \\input{tex/psis_loo_results.tex}\n",
    "}\n",
    "\\end{table}\n",
    "\n",
    "![PSIS-LOO k vales with different prior parameters](figs/k_values.png)\n",
    "\n",
    "\n",
    "According to the model selection results we parametrize our prior as $\\nu=6$ and $s=1$\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\theta) \\sim \\text{Student-t}(\\mu=0, \\nu=6, s=1)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "![Chosen prior](figs/best_prior.png)\n",
    "\n",
    "## Sensitivity Analysis\n",
    "\n",
    "As we don't have strong knowledge about the effect of features on survival probability, we choose a weakly informative prior. However, while choosing one prior distribution over others, we induce new modeling assumptions - this is true, regradless of whether we explicitely know what these assumptions are. Following the standard Bayesian Data Analysis convention, we investigate the sensitivity of posterior distribution towards the choice of the prior distribution by performing sensitivity analysis. \n",
    "\n",
    "The tables below show the observed posterior means and posterior 95\\% condifence interval spreads for selected features. The 95\\% posterior spread was calculated by substracting the lower limit from the upper limit of the confidence interval. As can be seen from the tables, the obtained feature means are relatively robust against the prior choice. However, there is some variation in the feature means, arguably correlating with increasing prior scale. For example, different priors assign age posterior means ranging from -1.74 to -2.58. Yet, the relative importance, or rank, the model assigns to different features, remains the same despite the choice of prior distribution.\n",
    "\n",
    "The 95 \\% confidence interval spread is not as robust against the prior choice. This can be seen when, for example, comparing the port of embarktion spreads between different priors. One intepretation is that the robustness of posterior confidence interval spread is correlated with the quality of the featurewise model-evidence. The more the posterior-distribution is constructed from data (the more it can be backed-up) the stabler it is.\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Sensitivity of the feature mean on prior choice}\n",
    "\\label{tab:mean_sensitivity}\n",
    "\\resizebox{0.75\\textwidth}{!}{\n",
    "    \\input{tex/mean_sensitivity.tex}\n",
    "}\n",
    "\\end{table}\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Sensitivity of the feature 95\\% confidence interval on the prior choice}\n",
    "\\label{tab:spread_sensitivity}\n",
    "\\resizebox{0.75\\textwidth}{!}{\n",
    "    \\input{tex/spread_sensitivity.tex}\n",
    "}\n",
    "\\end{table}\n",
    "\n",
    "\n",
    "\n",
    "## Convergence Diagnostic\n",
    "\n",
    "Based on the model selection the final predictive model was fitted with t(6,0,1) as prior, see table below for convergence diagnostics. Based on the convergence diagnostic we the model has convergenced well (R_hat < 1.01)\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Logistic regression converge statistics}\n",
    "\\resizebox{0.75\\textwidth}{!}{\n",
    "    \\input{tex/fit_logistic_regression.tex}\n",
    "}\n",
    "\\end{table}\n",
    "\n",
    "## Model Parameters\n",
    "\n",
    "Posterior distributions for the beta values of the regression model are presented in the figure below. The intepretation of the results is discussed in later sections. \n",
    "\n",
    "![Beta draws from posterior](figs/betas.png)\n",
    "\n",
    "Posterior predictice checking was conducted by drawing samples from posterior and comptuing the expected value for a singe draw. This value was then compared to the expected value from data. The posterior draws seem to model the data quite well, see figure below\n",
    "\n",
    "![Posterior predictive checking](figs/ppc.png)\n",
    "\n",
    "\n",
    "# Discussion\n",
    "\n",
    "Based on the beta values, the probability of survival is increased by being young and female and having a higher class ticket. This supports also the assumed policy of \"women and children first\". The intuitive fact that port of embarktion has no effect on survival is backed up by data. \n",
    "\n",
    "In addition to posterior predictive checking, we performed point-estimate based accuracy checking. Training and test error are presented in a table below. We can see that the logistic regression model achieves a significat improvement to a dummy model which predicts a person not surviving\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{F1-scores}\n",
    "\\resizebox{0.75\\textwidth}{!}{\n",
    "    \\input{tex/f1_scores.tex}\n",
    "}\n",
    "\\end{table}\n",
    "\n",
    "The most significant shortcomings of this model are related to the dataset as it had great deal of missing information. The missing information was not evenly distributed but concerned foremost third class passengers. The effect this bias had on the results is hard to estimate. \n",
    "\n",
    "# Conclusion\n",
    "\n",
    "We were able train a classifier to predict survivols with 0.74 F1 score and found out the intuitive fact that being young, female and having higher class ticket improved individuals chances to survive in titanic. The model predictions are not perfectly accurate but this was expected, as the survival included an irremovable element of randomness.\n",
    "\n",
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import sys\n",
    "import pystan\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pickle\n",
    "import seaborn as sn\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import image, pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from utilities import psis, stan_utility\n",
    "from utilities.my_utilities import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "CONVERT_TO_PDF = True\n",
    "RUN_MODEL_SELECTION = False\n",
    "RUN_SENSITIVITY_ANALYSIS = False\n",
    "\n",
    "figs = 'figs'\n",
    "tex = 'tex'\n",
    "\n",
    "if not os.path.exists(figs):\n",
    "    os.makedirs(figs)\n",
    "    \n",
    "if not os.path.exists(tex):\n",
    "    os.makedirs(tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "original_data = pd.read_csv(\"../data/titanic.txt\", index_col=\"name\")\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "dtypes = (original_data.dtypes.apply(lambda r: pd.Series({'dtype': 'numeric' if r in numerics else 'string'})))\n",
    "\n",
    "descriptions = [\n",
    "    'Name',\n",
    "    'Passenger class (1st, 2nd, 3rd)',\n",
    "    'Survival (0 = No; 1 = Yes)',\n",
    "    'Age',\n",
    "    'Port of Embarkation',\n",
    "    'Home/Destination',\n",
    "    'Cabin number',\n",
    "    'Ticket number',\n",
    "    'Lifeboat (number of NaN)',\n",
    "    'Sex (male, female)'\n",
    "]\n",
    "\n",
    "dtypes['description'] = descriptions\n",
    "\n",
    "with open(os.path.join(tex,'dtypes.tex'), 'w') as f:\n",
    "    f.write(dtypes.to_latex())\n",
    "    \n",
    "data = original_data.drop([\"row.names\",\n",
    "                           \"home.dest\", \n",
    "                           \"room\", \n",
    "                           \"ticket\", \n",
    "                           \"boat\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation between access to lifeboat and survival is 54.38%\n"
     ]
    }
   ],
   "source": [
    "were_on_lifeboat = original_data.boat.notna().astype('int')\n",
    "\n",
    "print(\"correlation between access to lifeboat and survival is {:.2%}\".format(were_on_lifeboat.corr(original_data.survived)))\n",
    "\n",
    "pclass_summary = data.groupby('pclass').agg({'survived': ['sum', 'mean'], \n",
    "                                             'age': [no_info, 'mean'], \n",
    "                                             'sex': [females, males, tot], \n",
    "                                             'embarked': [Queenstown, Cherbourg, Southampton, no_info, tot]\n",
    "})\n",
    "pclass_summary[\"tot\"] = data.pclass.value_counts()\n",
    "\n",
    "final = pd.DataFrame()\n",
    "final[\"Passengers\"] = pclass_summary[\"tot\"]\n",
    "final[\"Men\"] = pclass_summary[\"sex\"][\"males\"]\n",
    "final[\"Women\"] = pclass_summary[\"sex\"][\"females\"]\n",
    "final[\"Age (avg)\"] = pclass_summary[\"age\"][\"mean\"]\n",
    "final[\"Age unknown\"] = pclass_summary[\"age\"][\"no_info\"]\n",
    "final[\"Queenstown\"] = pclass_summary[\"embarked\"][\"Queenstown\"]\n",
    "final[\"Cherbourg\"] = pclass_summary[\"embarked\"][\"Cherbourg\"]\n",
    "final[\"Southampton\"] = pclass_summary[\"embarked\"][\"Southampton\"]\n",
    "final[\"Survived\"] = pclass_summary[\"survived\"][\"sum\"]\n",
    "final[\"Percentage\"] = pclass_summary[\"survived\"][\"mean\"]\n",
    "\n",
    "create_tex_table(final, os.path.join(tex,'key_stats.tex'))\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    display(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize categorical variables, drop NaNs and normalize and scale \"age\" between 0 and 1\n",
    "data_binarized = pd.get_dummies(data).dropna(axis=0, how=\"any\")\n",
    "data_binarized[\"child\"] = (data_binarized[\"age\"] < 15).astype(int)\n",
    "data_binarized[\"elderly\"] = (data_binarized[\"age\"] > 60).astype(int)\n",
    "data_binarized[\"age\"] = preprocessing.minmax_scale(preprocessing.scale(np.array(data_binarized[\"age\"])))\n",
    "\n",
    "corr_matrix = data_binarized.corr()\n",
    "create_tex_table(corr_matrix, os.path.join(tex, 'corr_matrix.tex'))\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    display(data_binarized.head(n=3))\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    heatmap = sn.heatmap(corr_matrix, cmap='coolwarm', ax=ax)\n",
    "    heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation = 35, fontsize = 12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figs, 'corr_matrix.png'), dpi=400)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 samples in training set \n",
      "209 samples in test set\n"
     ]
    }
   ],
   "source": [
    "# create arrays for a stan model\n",
    "features = [\"age\",\n",
    "            \"child\",\n",
    "            \"elderly\",\n",
    "            \"pclass_1st\", \n",
    "            \"pclass_2nd\", \n",
    "            \"pclass_3rd\", \n",
    "            \"embarked_Cherbourg\", \n",
    "            \"embarked_Queenstown\", \n",
    "            \"embarked_Southampton\", \n",
    "            \"sex_female\", \n",
    "            \"sex_male\"]\n",
    "\n",
    "y = np.array(data_binarized[\"survived\"])\n",
    "X = np.array(data_binarized[features], dtype=np.dtype(float))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(\"{0} samples in training set \\n{1} samples in test set\".format(y_train.size, y_test.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(prior_df, prior_s, debug=False):\n",
    "    model_data = dict(n=X_train.shape[0], \n",
    "                      d=X_train.shape[1], \n",
    "                      X=X_train, \n",
    "                      y=y_train, \n",
    "                      p_beta_df=prior_df, \n",
    "                      p_beta_scale=prior_s)\n",
    "    n_iter = 50 if debug else 2000\n",
    "    fit = model.sampling(data=model_data, seed=1, iter=n_iter, control=dict(max_treedepth=15))\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CONVERT_TO_PDF:\n",
    "    prior_dfs = [4, 5, 6]\n",
    "    prior_scale = [1, 4, 10, 20]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "    x = np.linspace(-20, 20)\n",
    "\n",
    "    for df in prior_dfs:\n",
    "        for s, ax in zip(prior_scale, axes.flat):\n",
    "            ax.plot(x, st.t.pdf(x, df=df, scale=s), label=r'$\\nu={}$'.format(df))\n",
    "\n",
    "            ax.legend(loc='best')\n",
    "            ax.set_title('Scale s = {}'.format(s))\n",
    "\n",
    "    fig.suptitle('Prior choice: Student-t distribution', fontsize=16, )\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "\n",
    "\n",
    "    plt.savefig(os.path.join(figs,'prior_choice.png'))\n",
    "    plt.savefig(os.path.join(figs,'prior_choice.eps'), format='eps', dpi=1000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    }
   ],
   "source": [
    "prior_dfs = [4, 5, 6]\n",
    "prior_scale = [1, 2, 4, 10, 20]\n",
    "model = stan_utility.compile_model('logistic_regression.stan')\n",
    "\n",
    "if RUN_MODEL_SELECTION:    \n",
    "    \n",
    "    datapoints = np.arange(1, X_train.shape[0] + 1)\n",
    "    \n",
    "    fig, axs = plt.subplots(len(prior_dfs), len(prior_scale), figsize=(17, 14))\n",
    "    axs = axs.ravel()\n",
    "    p_effs = []\n",
    "    loo_sums = []\n",
    "    Ks = []\n",
    "    i = 0\n",
    "\n",
    "    for df in tqdm(prior_dfs):\n",
    "        for s in prior_scale:\n",
    "            fit = fit_model(df, s)\n",
    "            samples = fit.extract(permuted=True)\n",
    "\n",
    "            # LOO CV\n",
    "            loo, loos, ks = psis.psisloo(samples[\"log_lik\"])\n",
    "            loo_sums.append(loo)\n",
    "            Ks.append(ks)\n",
    "            \n",
    "            lppd = np.sum(np.log(np.sum(np.exp(samples[\"log_lik\"]), axis=0)/4000))\n",
    "            p_effs.append(lppd-loo)\n",
    "\n",
    "            axs[i].plot(datapoints, ks, 'o')\n",
    "            axs[i].plot(datapoints, [0.7] * X_train.shape[0])\n",
    "            axs[i].set_title(\"prior t({0}, {1}, {2}) k-values\".format(df, 0, s))\n",
    "            i += 1\n",
    "            \n",
    "    fig.savefig(os.path.join(figs,'k_values.png'), bbox_inches='tight')\n",
    "    fig.savefig(os.path.join(figs,'k_values.eps'), bbox_inches='tight', format='eps', dpi=1000)\n",
    "    \n",
    "    with open(\"loo_sums.pkl\", 'wb') as f:\n",
    "        pickle.dump(loo_sums ,f, protocol=2)\n",
    "    with open(\"p_effs.pkl\", 'wb') as f:\n",
    "        pickle.dump(p_effs ,f, protocol=2)\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    img=image.imread(os.path.join(figs,'k_values.png'))\n",
    "    plt.figure(figsize = (50,50))\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RUN_MODEL_SELECTION:\n",
    "    with open(\"loo_sums.pkl\", 'rb') as f:\n",
    "        loo_sums = pickle.load(f, encoding='latin1')\n",
    "    with open(\"p_effs.pkl\", 'rb') as f:\n",
    "        p_effs = pickle.load(f, encoding='latin1')\n",
    "\n",
    "psis_loo_results = pd.DataFrame()\n",
    "psis_loo_results[\"p_eff\"] = p_effs\n",
    "psis_loo_results[\"PSIS-LOO\"] = loo_sums\n",
    "psis_loo_results[\"prior df\"] = [i for i, _ in itertools.product(prior_dfs, prior_scale)]\n",
    "psis_loo_results[\"prior scale\"] = [j for _, j in itertools.product(prior_dfs, prior_scale)]\n",
    "psis_loo_results_table = psis_loo_results.set_index([\"prior df\", \"prior scale\"], drop=True)\n",
    "create_tex_table(psis_loo_results_table, os.path.join(tex,'psis_loo_results.tex'))\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    display(psis_loo_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Predictive Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model\n",
    "largest_psis_loo_params = psis_loo_results[psis_loo_results[\"PSIS-LOO\"] == \n",
    "                                           np.max(psis_loo_results[\"PSIS-LOO\"])] \n",
    "if not CONVERT_TO_PDF:\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    df = int(largest_psis_loo_params[\"prior df\"])\n",
    "    scale = int(largest_psis_loo_params[\"prior scale\"])\n",
    "    ax.plot(x, st.t.pdf(x, df=df, scale=scale))\n",
    "    ax.set_title(r'Chosen prior distribution: Student-t($\\mu$=0, $\\nu$={}, s={})'.format(df, scale));\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figs,'best_prior.png'))\n",
    "    plt.savefig(os.path.join(figs,'best_prior.eps'), format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = fit_model(int(largest_psis_loo_params[\"prior df\"]), \n",
    "                int(largest_psis_loo_params[\"prior scale\"]))\n",
    "samples = fit.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ppc\n",
    "if not CONVERT_TO_PDF:\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    axs.hist(samples[\"p_hat_ppc\"], bins=42)\n",
    "    axs.set_title(\"Posterior Probability for Surviving, \\n Expected Value from Data {:.2f}\".format(np.mean(y_test)))\n",
    "    plt.show()\n",
    "    fig.savefig(os.path.join(figs,\"ppc.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot betas\n",
    "if not CONVERT_TO_PDF:\n",
    "    m = 4\n",
    "    n = int(math.ceil(len(features)/float(m)))\n",
    "    fig, axs = plt.subplots(n, m, figsize=(15, 12))\n",
    "    for i, (ax, feature) in enumerate(zip(axs.flat, features)):\n",
    "        ax.hist(samples[\"beta\"][:,i], bins=100)\n",
    "        ax.set_title(feature)\n",
    "    fig.savefig(os.path.join(figs,'betas.png'), bbox_inches='tight')\n",
    "    fig.savefig(os.path.join(figs,'betas.eps'), bbox_inches='tight', format='eps', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = ['alpha'] + ['beta[{}]'.format(i+1) for i in range(len(features))]\n",
    "fit_logistic_regression = create_fit_table(fit, os.path.join(tex,'fit_logistic_regression.tex'), filter=filt)\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    display(fit_logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    }
   ],
   "source": [
    "prior_dfs = [4, 5, 6] # redundant definition\n",
    "prior_scale = [1, 2, 4, 10, 20] # redundant definition\n",
    "model = stan_utility.compile_model('logistic_regression.stan') # redundant definition\n",
    "\n",
    "means = np.zeros((len(prior_dfs), len(prior_scale), len(features) + 1))\n",
    "intervals = np.zeros_like(means, dtype=(float, 2))\n",
    "\n",
    "if RUN_SENSITIVITY_ANALYSIS:\n",
    "    \n",
    "    for i, df in enumerate(tqdm(prior_dfs)):\n",
    "        for j, s in enumerate(prior_scale):\n",
    "            fit = fit_model(df, s, debug=False)\n",
    "            \n",
    "            # filter alpha and betas\n",
    "            filt = ['alpha'] + ['beta[{}]'.format(i+1) for i in range(len(features))]\n",
    "            filtered = create_fit_table(fit, filter=filt) \n",
    "            \n",
    "            means[i, j, :] = filtered['mean']\n",
    "            tuples = [t for t in zip(filtered['2.5%'], filtered['97.5%'])]\n",
    "            intervals[i, j, :] = tuples\n",
    "    \n",
    "    multi_index = pd.MultiIndex.from_product([prior_dfs, prior_scale], names=['prior_df', 'prior_scale'])\n",
    "\n",
    "    mean_sensitivity = pd.Panel(means).transpose(2, 0, 1).to_frame()\n",
    "    mean_sensitivity.index = multi_index\n",
    "    mean_sensitivity.columns = ['alpha'] + features\n",
    "\n",
    "    spread = np.subtract(intervals[:,:,:,1], intervals[:,:,:,0])\n",
    "    spread_sensitivity = pd.Panel(spread).transpose(2, 0, 1).to_frame()\n",
    "    spread_sensitivity.index = multi_index\n",
    "    spread_sensitivity.columns = ['alpha'] + features\n",
    "    \n",
    "    with open(\"sensitivity_mean.pkl\", 'wb') as f:\n",
    "        pickle.dump(mean_sensitivity ,f, protocol=2)\n",
    "    with open(\"sensitivity_spread.pkl\", 'wb') as f:\n",
    "        pickle.dump(spread_sensitivity ,f, protocol=2)\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RUN_SENSITIVITY_ANALYSIS:\n",
    "    with open('sensitivity_mean.pkl', 'rb') as f:\n",
    "        mean_sensitivity = pickle.load(f, encoding='latin1')\n",
    "    with open('sensitivity_spread.pkl', 'rb') as f:\n",
    "        spread_sensitivity = pickle.load(f, encoding='latin1')\n",
    "\n",
    "create_tex_table(spread_sensitivity[[f for f in features if 'embarked' not in f]], os.path.join(tex,'spread_sensitivity.tex'))    \n",
    "create_tex_table(mean_sensitivity[[f for f in features if 'embarked' not in f]], os.path.join(tex,'mean_sensitivity.tex'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Performance Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x, beta, alpha):\n",
    "    return (1+np.exp(-(alpha + np.dot(x, beta))))**(-1)\n",
    "\n",
    "def get_y_preds(data, beta, alpha):\n",
    "    y_preds = []\n",
    "    for x in data:\n",
    "        res = logistic(x, beta, alpha)\n",
    "        y_preds.append(1 if res > 0.5 else 0)\n",
    "    return y_preds\n",
    "        \n",
    "def check_accuracy(data, target, beta, alpha):\n",
    "    ans_list = []\n",
    "    for i in range(len(data)):\n",
    "        res = logistic(data[i], beta, alpha)\n",
    "        ans = 1 if res > 0.5 else 0\n",
    "        ans_list.append(ans == target[i])\n",
    "\n",
    "    return np.mean(ans_list)\n",
    "\n",
    "def check_dummy_accuracy(target, res):\n",
    "    ans_list = []\n",
    "    for i in range(len(res)):\n",
    "        ans_list.append(res[i] == target[i])\n",
    "    return np.mean(ans_list)\n",
    "\n",
    "mean_list = fit.summary()[\"summary\"]\n",
    "beta = mean_list[1:len(features)+1, 0]\n",
    "alpha = mean_list[0, 0]\n",
    "\n",
    "predictive_performance = pd.DataFrame()\n",
    "predictive_performance[\"Dummy (All 1) training\"] = [f1_score(y_train, [1] * len(y_train))]\n",
    "predictive_performance[\"training\"] = [f1_score(y_train, get_y_preds(X_train, beta, alpha))]\n",
    "predictive_performance[\"Dummy (All 1) test\"] = [f1_score(y_test, [1] * len(y_test))]\n",
    "predictive_performance[\"test\"] = [f1_score(y_test, get_y_preds(X_test, beta, alpha))]\n",
    "\n",
    "create_tex_table(predictive_performance, os.path.join(tex,'f1_scores.tex'))\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    display(predictive_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### my_utilities.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\lstinputlisting{utilities/my_utilities.py}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic_regression.stan"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\lstinputlisting{logistic_regression.stan}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
