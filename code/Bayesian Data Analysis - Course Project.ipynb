{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting survivols for the Titanic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "This project aims to examine these differences in survival probability in depth. We hope to create an accurate model for predicting individual level survival probabilities. We chose survival status as our dependent variable and age, passenger class, sex, and embarkment city to be our independent variables. We aim at learning new insights on how these features affected the survival probablity of the participants in this tragedy that shook the world. \n",
    "\n",
    "## Data\n",
    "\n",
    "The dataset we worked on was ready-made and aquired from Vanderbilt Biostatistics dataset collection (http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.html). The dataset relies on multiple sources major one being the Encyclopedia Titanica webpage. It contains information about all 1313 passengers including their whole names, sex, age, passenger class, and survival status among others. The dataset does not include information about the RMS Titanic crew. RMS Titanic had three different passenger classes.\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "The dataset required preprocesing before it could be used in modelling. First, categorical variables (sex, passenger class, embarkment city) were transformed into binary form. Second, all rows including missing values were dropped out. It is notable that this removes major part of the original data. Moreover, has different effect on different passenger classes as over 70% of 3rd class passenger data is lost. Third, age variable was scaled and normalized to avoid its larger magnitude to skew results. Finally, data set was divided into training (67 %) and test (33 %) set.\n",
    "\n",
    "### Key Statistics\n",
    "\n",
    "Key statistics were computed for the dataset. The table below presenst the key statistics. \n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Key statistics by passenger class}\n",
    "\\resizebox{0.95\\textwidth}{!}{\n",
    "    \\input{key_stats.tex}\n",
    "}\n",
    "\\end{table}\n",
    "\n",
    "\n",
    "## Model\n",
    "\n",
    "Our choice is to examine the data with logistic regression. In logistic regression linear regression is nonlinearized with a logistic function. The function limits regression values between 0 and 1 and thus provides a prozy for probability of survival.\n",
    "\n",
    "### Prior choice\n",
    "\n",
    "As we don't have prior knowledge on how the data is distributed, a weakly informative prior is chosen according to stan development team guidelines: student_t(nu,0,s) distribution, where s is chosen to provide weak information on the expected scale, and 3<nu<7.\n",
    "\n",
    "### Model Selection\n",
    "\n",
    "This prior further adjusted in model selection. Several different combinations of degrees of freedom and scale values for prior were compared by applying PSIS-LOO cross validation to the logistic model. The table and the figure below present results of the cross validation. It can be seen that prior with 6 degrees of freedom and scale as 1 provides the best fit based on the PSIS-LOO value and corresponding k values.\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{PSIS-LOO cross validation results}\n",
    "\\resizebox{0.45\\textwidth}{!}{\n",
    "    \\input{psis_loo_results.tex}\n",
    "}\n",
    "\\end{table}\n",
    "\n",
    "![PSIS-LOO k vales with different prior parameters](k_values.png)\n",
    "\n",
    "### Sensitivity Analysis\n",
    "\n",
    "### Convergence Diagnostic\n",
    "\n",
    "Based on the model selection the final predictive model was fitted with t(6,0,1) as prior, see table below for convergence diagnostics. Based on the convergence diagnostic we the model has convergenced well (R_hat < 1.01)\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Logistic regression converge statistics}\n",
    "\\resizebox{0.75\\textwidth}{!}{\n",
    "    \\input{fit_logistic_regression.tex}\n",
    "}\n",
    "\\end{table}\n",
    "\n",
    "### Model Parameters\n",
    "Betas of the regression model are presented in the figure below.\n",
    "![Beta draws from posterior](betas.png)\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "Based on the beta values, the probability of survival is increased by being young and female and having a higher class ticket. The embarkment city seems not have any effect which is pretty intuitive. \n",
    "\n",
    "Training and test error are presented in a table below. We can see that the logistic regression model achieves a significat improvement to a dummy model which predicts a person not surviving\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{F1-scores}\n",
    "\\resizebox{0.75\\textwidth}{!}{\n",
    "    \\input{f1_scores.tex}\n",
    "}\n",
    "\\end{table}\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We were able train a classifier to predict survivols with 0.74 F1 score and found out the intuitive fact that being young, female and having higher class ticket improved individuals changes to survive in titanic\n",
    "\n",
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import sys\n",
    "import pystan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import image, pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from utilities import psis, stan_utility\n",
    "from utilities.my_utilities import *\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERT_TO_PDF = True\n",
    "RUN_MODEL_SELECTION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "data = pd.read_csv(\"../data/titanic.txt\", index_col=\"name\").drop([\"row.names\", \n",
    "                                                                  \"home.dest\", \n",
    "                                                                  \"room\", \n",
    "                                                                  \"ticket\", \n",
    "                                                                  \"boat\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclass_summary = data.groupby('pclass').agg({'survived': ['sum', 'mean'], \n",
    "                                             'age': [no_info, 'mean'], \n",
    "                                             'sex': [females, males, tot], \n",
    "                                             'embarked': [Queenstown, Cherbourg, Southampton, no_info, tot]\n",
    "})\n",
    "pclass_summary[\"tot\"] = data.pclass.value_counts()\n",
    "\n",
    "final = pd.DataFrame()\n",
    "final[\"Passengers\"] = pclass_summary[\"tot\"]\n",
    "final[\"Men\"] = pclass_summary[\"sex\"][\"males\"]\n",
    "final[\"Women\"] = pclass_summary[\"sex\"][\"females\"]\n",
    "final[\"Age (avg)\"] = pclass_summary[\"age\"][\"mean\"]\n",
    "final[\"Age unknown\"] = pclass_summary[\"age\"][\"no_info\"]\n",
    "final[\"Queenstown\"] = pclass_summary[\"embarked\"][\"Queenstown\"]\n",
    "final[\"Cherbourg\"] = pclass_summary[\"embarked\"][\"Cherbourg\"]\n",
    "final[\"Southampton\"] = pclass_summary[\"embarked\"][\"Southampton\"]\n",
    "final[\"Survived\"] = pclass_summary[\"survived\"][\"sum\"]\n",
    "final[\"Percentage\"] = pclass_summary[\"survived\"][\"mean\"]\n",
    "\n",
    "create_tex_table(final, 'key_stats.tex')\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    display(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize categorical variables, drop NaNs and normalize and scale \"age\" between 0 and 1\n",
    "data_binarized = pd.get_dummies(data).dropna(axis=0, how=\"any\")\n",
    "data_binarized[\"child\"] = (data_binarized[\"age\"] < 15).astype(int)\n",
    "data_binarized[\"elderly\"] = (data_binarized[\"age\"] > 60).astype(int)\n",
    "data_binarized[\"age\"] = preprocessing.minmax_scale(preprocessing.scale(np.array(data_binarized[\"age\"])))\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    display(data_binarized.head(n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 samples in training set \n",
      "209 samples in test set\n"
     ]
    }
   ],
   "source": [
    "# create arrays for a stan model\n",
    "features = [\"age\",\n",
    "            \"child\",\n",
    "            \"elderly\",\n",
    "            \"pclass_1st\", \n",
    "            \"pclass_2nd\", \n",
    "            \"pclass_3rd\", \n",
    "            \"embarked_Cherbourg\", \n",
    "            \"embarked_Queenstown\", \n",
    "            \"embarked_Southampton\", \n",
    "            \"sex_female\", \n",
    "            \"sex_male\"]\n",
    "\n",
    "y = np.array(data_binarized[\"survived\"])\n",
    "X = np.array(data_binarized[features], dtype=np.dtype(float))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(\"{0} samples in training set \\n{1} samples in test set\".format(y_train.size, y_test.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(prior_df, prior_s):\n",
    "    model_data = dict(n=X_train.shape[0], \n",
    "                      d=X_train.shape[1], \n",
    "                      X=X_train, \n",
    "                      y=y_train, \n",
    "                      p_beta_df=prior_df, \n",
    "                      p_beta_scale=prior_s)\n",
    "    fit = model.sampling(data=model_data, seed=1, control=dict(max_treedepth=15))\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    }
   ],
   "source": [
    "prior_dfs = [4, 5, 6]\n",
    "prior_scale = [1, 2, 4, 10, 20]\n",
    "model = stan_utility.compile_model('logistic_regression.stan')\n",
    "\n",
    "if RUN_MODEL_SELECTION:    \n",
    "    fig, axs = plt.subplots(len(prior_dfs), len(prior_scale), figsize=(17, 14))\n",
    "    axs = axs.ravel()\n",
    "    p_effs = []\n",
    "    loo_sums = []\n",
    "    i = 0\n",
    "\n",
    "    for df in tqdm(prior_dfs):\n",
    "        for s in prior_scale:\n",
    "            fit = fit_model(df, s)\n",
    "            samples = fit.extract(permuted=True)\n",
    "\n",
    "            # LOO CV\n",
    "            loo, loos, ks = psis.psisloo(samples[\"log_lik\"])\n",
    "            loo_sums.append(loo)\n",
    "\n",
    "            lppd = np.sum(np.log(np.sum(np.exp(samples[\"log_lik\"]), axis=0)/4000))\n",
    "            p_effs.append(lppd-loo)\n",
    "\n",
    "            datapoints = np.arange(1, X_train.shape[0] + 1)\n",
    "            axs[i].plot(datapoints, ks, 'o')\n",
    "            axs[i].plot(datapoints, [0.7] * X_train.shape[0])\n",
    "            axs[i].set_title(\"prior t({0}, {1}, {2}) k-values\".format(df, 0, s))\n",
    "            i += 1\n",
    "            \n",
    "    fig.savefig('k_values.png', bbox_inches='tight')\n",
    "    with open(\"loo_sums.pkl\", 'wb') as f:\n",
    "        pickle.dump(loo_sums ,f, protocol=2)\n",
    "    with open(\"p_effs.pkl\", 'wb') as f:\n",
    "        pickle.dump(p_effs ,f, protocol=2)\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    img=image.imread('k_values.png')\n",
    "    plt.figure(figsize = (50,50))\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RUN_MODEL_SELECTION:\n",
    "    with open(\"loo_sums.pkl\", 'rb') as f:\n",
    "        loo_sums = pickle.load(f)\n",
    "    with open(\"p_effs.pkl\", 'rb') as f:\n",
    "        p_effs = pickle.load(f)\n",
    "\n",
    "psis_loo_results = pd.DataFrame()\n",
    "psis_loo_results[\"p_eff\"] = p_effs\n",
    "psis_loo_results[\"PSIS-LOO\"] = loo_sums\n",
    "psis_loo_results[\"prior df\"] = [i for i, _ in itertools.product(prior_dfs, prior_scale)]\n",
    "psis_loo_results[\"prior scale\"] = [j for _, j in itertools.product(prior_dfs, prior_scale)]\n",
    "psis_loo_results_table = psis_loo_results.set_index([\"prior df\", \"prior scale\"], drop=True)\n",
    "create_tex_table(psis_loo_results_table, 'psis_loo_results.tex')\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    display(psis_loo_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot betas\n",
    "largest_psis_loo_params = psis_loo_results[psis_loo_results[\"PSIS-LOO\"] == \n",
    "                                           np.max(psis_loo_results[\"PSIS-LOO\"])] \n",
    "fit = fit_model(int(largest_psis_loo_params[\"prior df\"]), \n",
    "                int(largest_psis_loo_params[\"prior scale\"]))\n",
    "samples = fit.extract(permuted=True)\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    m = 4\n",
    "    n = int(math.ceil(len(features)/float(m)))\n",
    "    fig, axs = plt.subplots(n, m, figsize=(15, 12))\n",
    "    for i, (ax, feature) in enumerate(zip(axs.flat, features)):\n",
    "        ax.hist(samples[\"beta\"][:,i], bins=100)\n",
    "        ax.set_title(feature)\n",
    "    fig.savefig('betas.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = ['alpha'] + ['beta[{}]'.format(i+1) for i in range(len(features))]\n",
    "fit_logistic_regression = create_fit_table(fit, 'fit_logistic_regression.tex', filter=filt)\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    display(fit_logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Performance Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x, beta, alpha):\n",
    "    return (1+np.exp(-(alpha + np.dot(x, beta))))**(-1)\n",
    "\n",
    "def get_y_preds(data, beta, alpha):\n",
    "    y_preds = []\n",
    "    for x in data:\n",
    "        res = logistic(x, beta, alpha)\n",
    "        y_preds.append(1 if res > 0.5 else 0)\n",
    "    return y_preds\n",
    "        \n",
    "def check_accuracy(data, target, beta, alpha):\n",
    "    ans_list = []\n",
    "    for i in range(len(data)):\n",
    "        res = logistic(data[i], beta, alpha)\n",
    "        ans = 1 if res > 0.5 else 0\n",
    "        ans_list.append(ans == target[i])\n",
    "\n",
    "    return np.mean(ans_list)\n",
    "\n",
    "def check_dummy_accuracy(target, res):\n",
    "    ans_list = []\n",
    "    for i in range(len(res)):\n",
    "        ans_list.append(res[i] == target[i])\n",
    "    return np.mean(ans_list)\n",
    "\n",
    "mean_list = fit.summary()[\"summary\"]\n",
    "beta = mean_list[1:len(features)+1, 0]\n",
    "alpha = mean_list[0, 0]\n",
    "\n",
    "predictive_performance = pd.DataFrame()\n",
    "predictive_performance[\"Dummy (All 1) training\"] = [f1_score(y_train, [1] * len(y_train))]\n",
    "predictive_performance[\"training\"] = [f1_score(y_train, get_y_preds(X_train, beta, alpha))]\n",
    "predictive_performance[\"Dummy (All 1) test\"] = [f1_score(y_test, [1] * len(y_test))]\n",
    "predictive_performance[\"test\"] = [f1_score(y_test, get_y_preds(X_test, beta, alpha))]\n",
    "\n",
    "create_tex_table(predictive_performance, 'f1_scores.tex')\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    display(predictive_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### my_utilities.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\lstinputlisting{utilities/my_utilities.py}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic_regression.stan"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\lstinputlisting{logistic_regression.stan}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
