{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting survivols for the Titanic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "This project aims to examine these differences in survival probability in depth. We hope to create an accurate model for predicting individual level survival probabilities. We chose survival status as our dependent variable and age, passenger class, sex, and embarkment city to be our independent variables. We aim at learning new insights on how these features affected the survival probablity of the participants in this tragedy that shook the world. \n",
    "\n",
    "## Data\n",
    "\n",
    "The dataset we worked on was ready-made and aquired from Vanderbilt Biostatistics dataset collection (http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.html). The dataset relies on multiple sources major one being the Encyclopedia Titanica webpage. It contains information about all 1313 passengers including their whole names, sex, age, passenger class, and survival status among others. The dataset does not include information about the RMS Titanic crew. RMS Titanic had three different passenger classes.\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Data features}\n",
    "\\input{dtypes.tex}\n",
    "\\end{table}\n",
    "\n",
    "We chose to investigate the effect of sex, age, passenger class, and port of embarkment on the survival probability. We excluded the variable denoting wheter individual was on a lifeboat as we aimed to find the underlying reasons, why some people ended up in lifeboats and others not. In addition, we wanted to test whether being child or an elderly had any effect. The motivation behind this addition was to figure out if the assumed policy of \"women and children first\" can be backed up by the data. To do so, we derived two additional features, child and elderly, denoting passengers under the age of 15 and over the age of 65, respectively.\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "The dataset required preprocesing before it could be used in modelling. First, categorical variables (sex, passenger class, embarkment city) were transformed into binary form. Second, all rows including missing values were dropped out. It is notable that this removes major part of the original data. Moreover, has different effect on different passenger classes as over 70% of 3rd class passenger data is lost. Third, age variable was scaled and normalized to avoid its larger magnitude to skew results. Finally, data set was divided into training (67 %) and test (33 %) set.\n",
    "\n",
    "### Key Statistics\n",
    "\n",
    "Key statistics were computed for the dataset passenger class-wise. The table below presenst the key statistics. \n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Key statistics by passenger class}\n",
    "\\resizebox{0.95\\textwidth}{!}{\n",
    "    \\input{key_stats.tex}\n",
    "}\n",
    "\\end{table}\n",
    "\n",
    "The likelihoods of survival differ between different passenger classes. 60\\% of the first class passengers survived, as compared to 19\\% of the third class passengers. 711 of the passengers were travelling in the third class which is more than first and second class passengers combined. However, most of the third class information is missing which might introduce bias to the model. First class passengers were evidently older than second and third class passengers, which might increase the correlation betweem age and surivaval probability. \n",
    "\n",
    "## Model\n",
    "\n",
    "Our choice is to examine the data with logistic regression. In logistic regression linear regression is nonlinearized with a logistic function. The function limits regression values between 0 and 1 and thus provides a prozy for probability of survival.\n",
    "\n",
    "### Prior choice\n",
    "\n",
    "As we don't have prior knowledge on how the data is distributed, a weakly informative prior is chosen according to stan development team guidelines. A Student-t distribution with zero mean is a suitable choice for logistic regression (see i.e. https://arxiv.org/pdf/0901.4011.pdf for more information). Thus, for each regression coefficient, we assume a Student-t prior with mean 0, degrees-of-freedom parameter $\\nu$ within the range $3 < \\nu < 7$, and scale $s$, where $s$ is chosen to provide weak information on the expected scale. \n",
    "\n",
    "![Student-t distribution with different scales and degrees of freedom](prior_choice.png)\n",
    "\n",
    "### Model Selection\n",
    "\n",
    "This prior further adjusted in model selection. Several different combinations of degrees of freedom and scale values for prior were compared by applying PSIS-LOO cross validation to the logistic model. The table and the figure below present results of the cross validation. It can be seen that prior with 6 degrees of freedom and scale as 1 provides the best fit based on the PSIS-LOO value and corresponding k values.\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\theta) \\sim Student-(\\mu=0, \\nu=6, s=1)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{PSIS-LOO cross validation results}\n",
    "\\resizebox{0.45\\textwidth}{!}{\n",
    "    \\input{psis_loo_results.tex}\n",
    "}\n",
    "\\end{table}\n",
    "\n",
    "![PSIS-LOO k vales with different prior parameters](k_values.png)\n",
    "![Chosen prior](best_prior.png)\n",
    "\n",
    "### Sensitivity Analysis\n",
    "\n",
    "As we don't have strong knowledge about the effect of features on the survival probability, we choose a weakly informative prior. However, while choosing one prior distribution over others, we induce new modeling assumptions - this is true, regradless whether we explicitely know what these assumptions are. Following the standard Bayesian Data Analysis convention, we investigate the sensitivity of posterior distribution on the choice of the prior distribution by performing sensitivity analysis. We hypothesize that the effect prior choice has on posterior distribution is small, as all our prior candidates are weakly informative and our dataset relatively large (> 1000 rows). \n",
    "\n",
    "The tables below show the observed posterior means and posterior 95\\% condifence interval spreads for selected features. The 95 \\% posterior spread was calculated by substracting the lower point of the confidence interval from the upper point. As can be seen from the tables, the obtained feature means are relatively robust against the prior choice. However, there is some variation in the feature means, apparently correlating with increasing prior scale. For example, the age variable is assigned means ranging from -1.74 to -2.58 by the use of different priors. Yet, the relative importance the model assign to different features remains same despite the choice of prior distribution.\n",
    "\n",
    "The 95 \\% confidence interval spread is not as robust against the prior choice. This can be seen when, for example, comparing the embarkment city spreads between different priors. One intepretation is that the robustness of posterior confidence interval spread is correlated with the quality of the feature-wise model-evidence. The more the posterior-distribution is constructed from data (the more it can be backed-up) the stabler it is.\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Sensitivity of the feature mean on prior choice}\n",
    "\\label{tab:mean_sensitivity}\n",
    "\\resizebox{0.75\\textwidth}{!}{\n",
    "    \\input{mean_sensitivity.tex}\n",
    "}\n",
    "\\end{table}\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Sensitivity of the feature 95\\% confidence interval on the prior choice}\n",
    "\\label{tab:spread_sensitivity}\n",
    "\\resizebox{0.75\\textwidth}{!}{\n",
    "    \\input{spread_sensitivity.tex}\n",
    "}\n",
    "\\end{table}\n",
    "\n",
    "\n",
    "\n",
    "### Convergence Diagnostic\n",
    "\n",
    "Based on the model selection the final predictive model was fitted with t(6,0,1) as prior, see table below for convergence diagnostics. Based on the convergence diagnostic we the model has convergenced well (R_hat < 1.01)\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Logistic regression converge statistics}\n",
    "\\resizebox{0.75\\textwidth}{!}{\n",
    "    \\input{fit_logistic_regression.tex}\n",
    "}\n",
    "\\end{table}\n",
    "\n",
    "### Model Parameters\n",
    "Betas of the regression model are presented in the figure below. Posterior predictice checking was conducted by drawing samples from posterior and comptuing the expected value for a singe draw. This value was then compared to the expected value from data. The posterior draws seem to model the data quite well, see figure below\n",
    "![Beta draws from posterior](betas.png)\n",
    "![title](ppc.png)\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "Based on the beta values, the probability of survival is increased by being young and female and having a higher class ticket. The embarkment city seems not have any effect which is pretty intuitive. \n",
    "\n",
    "Training and test error are presented in a table below. We can see that the logistic regression model achieves a significat improvement to a dummy model which predicts a person not surviving\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{F1-scores}\n",
    "\\resizebox{0.75\\textwidth}{!}{\n",
    "    \\input{f1_scores.tex}\n",
    "}\n",
    "\\end{table}\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We were able train a classifier to predict survivols with 0.74 F1 score and found out the intuitive fact that being young, female and having higher class ticket improved individuals changes to survive in titanic\n",
    "\n",
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import sys\n",
    "import pystan\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import image, pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from utilities import psis, stan_utility\n",
    "from utilities.my_utilities import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "CONVERT_TO_PDF = True\n",
    "RUN_MODEL_SELECTION = False\n",
    "RUN_SENSITIVITY_ANALYSIS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "original_data = pd.read_csv(\"../data/titanic.txt\", index_col=\"name\")\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "dtypes = (original_data.dtypes.apply(lambda r: pd.Series({'dtype': 'numeric' if r in numerics else 'string'})))\n",
    "\n",
    "descriptions = [\n",
    "    'Name',\n",
    "    'Passenger class (1st, 2nd, 3rd)',\n",
    "    'passrnger survival (0 = No; 1 = Yes)',\n",
    "    'Age',\n",
    "    'Port of Embarkation',\n",
    "    'Home/Destination',\n",
    "    'Cabin number',\n",
    "    'Ticket number',\n",
    "    'Lifeboat (number of NaN)',\n",
    "    'Sex (male, female)'\n",
    "]\n",
    "dtypes['desctiption'] = descriptions\n",
    "with open('dtypes.tex', 'w') as f:\n",
    "    f.write(dtypes.to_latex())\n",
    "    \n",
    "data = original_data.drop([\"row.names\",\n",
    "                           \"home.dest\", \n",
    "                           \"room\", \n",
    "                           \"ticket\", \n",
    "                           \"boat\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data['were_on_lifeboat'] = original_data.boat.notna().astype('int')\n",
    "create_tex_table(original_data.corr().drop('row.names').drop('row.names', axis=1), 'confusion.tex')\n",
    "\n",
    "\n",
    "pclass_summary = data.groupby('pclass').agg({'survived': ['sum', 'mean'], \n",
    "                                             'age': [no_info, 'mean'], \n",
    "                                             'sex': [females, males, tot], \n",
    "                                             'embarked': [Queenstown, Cherbourg, Southampton, no_info, tot]\n",
    "})\n",
    "pclass_summary[\"tot\"] = data.pclass.value_counts()\n",
    "\n",
    "final = pd.DataFrame()\n",
    "final[\"Passengers\"] = pclass_summary[\"tot\"]\n",
    "final[\"Men\"] = pclass_summary[\"sex\"][\"males\"]\n",
    "final[\"Women\"] = pclass_summary[\"sex\"][\"females\"]\n",
    "final[\"Age (avg)\"] = pclass_summary[\"age\"][\"mean\"]\n",
    "final[\"Age unknown\"] = pclass_summary[\"age\"][\"no_info\"]\n",
    "final[\"Queenstown\"] = pclass_summary[\"embarked\"][\"Queenstown\"]\n",
    "final[\"Cherbourg\"] = pclass_summary[\"embarked\"][\"Cherbourg\"]\n",
    "final[\"Southampton\"] = pclass_summary[\"embarked\"][\"Southampton\"]\n",
    "final[\"Survived\"] = pclass_summary[\"survived\"][\"sum\"]\n",
    "final[\"Percentage\"] = pclass_summary[\"survived\"][\"mean\"]\n",
    "\n",
    "create_tex_table(final, 'key_stats.tex')\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    display(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize categorical variables, drop NaNs and normalize and scale \"age\" between 0 and 1\n",
    "data_binarized = pd.get_dummies(data).dropna(axis=0, how=\"any\")\n",
    "data_binarized[\"child\"] = (data_binarized[\"age\"] < 15).astype(int)\n",
    "data_binarized[\"elderly\"] = (data_binarized[\"age\"] > 60).astype(int)\n",
    "data_binarized[\"age\"] = preprocessing.minmax_scale(preprocessing.scale(np.array(data_binarized[\"age\"])))\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    display(data_binarized.head(n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 samples in training set \n",
      "209 samples in test set\n"
     ]
    }
   ],
   "source": [
    "# create arrays for a stan model\n",
    "features = [\"age\",\n",
    "            \"child\",\n",
    "            \"elderly\",\n",
    "            \"pclass_1st\", \n",
    "            \"pclass_2nd\", \n",
    "            \"pclass_3rd\", \n",
    "            \"embarked_Cherbourg\", \n",
    "            \"embarked_Queenstown\", \n",
    "            \"embarked_Southampton\", \n",
    "            \"sex_female\", \n",
    "            \"sex_male\"]\n",
    "\n",
    "y = np.array(data_binarized[\"survived\"])\n",
    "X = np.array(data_binarized[features], dtype=np.dtype(float))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(\"{0} samples in training set \\n{1} samples in test set\".format(y_train.size, y_test.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(prior_df, prior_s, debug=False):\n",
    "    model_data = dict(n=X_train.shape[0], \n",
    "                      d=X_train.shape[1], \n",
    "                      X=X_train, \n",
    "                      y=y_train, \n",
    "                      p_beta_df=prior_df, \n",
    "                      p_beta_scale=prior_s)\n",
    "    n_iter = 50 if debug else 2000\n",
    "    fit = model.sampling(data=model_data, seed=1, iter=n_iter, control=dict(max_treedepth=15))\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CONVERT_TO_PDF:\n",
    "    prior_dfs = [4, 5, 6]\n",
    "    prior_scale = [1, 4, 10, 20]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "    x = np.linspace(-20, 20)\n",
    "\n",
    "    for df in prior_dfs:\n",
    "        for s, ax in zip(prior_scale, axes.flat):\n",
    "            ax.plot(x, st.t.pdf(x, df=df, scale=s), label=r'$\\nu={}$'.format(df))\n",
    "\n",
    "            ax.legend(loc='best')\n",
    "            ax.set_title('Scale s = {}'.format(s))\n",
    "\n",
    "    fig.suptitle('Prior choice: Student-t distribution', fontsize=16, )\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "\n",
    "\n",
    "    plt.savefig('prior_choice.png')\n",
    "    plt.savefig('prior_choice.eps', format='eps', dpi=1000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    }
   ],
   "source": [
    "prior_dfs = [4, 5, 6]\n",
    "prior_scale = [1, 2, 4, 10, 20]\n",
    "model = stan_utility.compile_model('logistic_regression.stan')\n",
    "\n",
    "if RUN_MODEL_SELECTION:    \n",
    "    \n",
    "    datapoints = np.arange(1, X_train.shape[0] + 1)\n",
    "    \n",
    "    fig, axs = plt.subplots(len(prior_dfs), len(prior_scale), figsize=(17, 14))\n",
    "    axs = axs.ravel()\n",
    "    p_effs = []\n",
    "    loo_sums = []\n",
    "    Ks = []\n",
    "    i = 0\n",
    "\n",
    "    for df in tqdm(prior_dfs):\n",
    "        for s in prior_scale:\n",
    "            fit = fit_model(df, s)\n",
    "            samples = fit.extract(permuted=True)\n",
    "\n",
    "            # LOO CV\n",
    "            loo, loos, ks = psis.psisloo(samples[\"log_lik\"])\n",
    "            loo_sums.append(loo)\n",
    "            Ks.append(ks)\n",
    "            \n",
    "            lppd = np.sum(np.log(np.sum(np.exp(samples[\"log_lik\"]), axis=0)/4000))\n",
    "            p_effs.append(lppd-loo)\n",
    "\n",
    "            axs[i].plot(datapoints, ks, 'o')\n",
    "            axs[i].plot(datapoints, [0.7] * X_train.shape[0])\n",
    "            axs[i].set_title(\"prior t({0}, {1}, {2}) k-values\".format(df, 0, s))\n",
    "            i += 1\n",
    "            \n",
    "    fig.savefig('k_values.png', bbox_inches='tight')\n",
    "    fig.savefig('k_values.eps', bbox_inches='tight', format='eps', dpi=1000)\n",
    "    \n",
    "    with open(\"loo_sums.pkl\", 'wb') as f:\n",
    "        pickle.dump(loo_sums ,f, protocol=2)\n",
    "    with open(\"p_effs.pkl\", 'wb') as f:\n",
    "        pickle.dump(p_effs ,f, protocol=2)\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    img=image.imread('k_values.png')\n",
    "    plt.figure(figsize = (50,50))\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RUN_MODEL_SELECTION:\n",
    "    with open(\"loo_sums.pkl\", 'rb') as f:\n",
    "        loo_sums = pickle.load(f, encoding='latin1')\n",
    "    with open(\"p_effs.pkl\", 'rb') as f:\n",
    "        p_effs = pickle.load(f, encoding='latin1')\n",
    "\n",
    "psis_loo_results = pd.DataFrame()\n",
    "psis_loo_results[\"p_eff\"] = p_effs\n",
    "psis_loo_results[\"PSIS-LOO\"] = loo_sums\n",
    "psis_loo_results[\"prior df\"] = [i for i, _ in itertools.product(prior_dfs, prior_scale)]\n",
    "psis_loo_results[\"prior scale\"] = [j for _, j in itertools.product(prior_dfs, prior_scale)]\n",
    "psis_loo_results_table = psis_loo_results.set_index([\"prior df\", \"prior scale\"], drop=True)\n",
    "create_tex_table(psis_loo_results_table, 'psis_loo_results.tex')\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    display(psis_loo_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Predictive Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model\n",
    "largest_psis_loo_params = psis_loo_results[psis_loo_results[\"PSIS-LOO\"] == \n",
    "                                           np.max(psis_loo_results[\"PSIS-LOO\"])] \n",
    "if not CONVERT_TO_PDF:\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    df = int(largest_psis_loo_params[\"prior df\"])\n",
    "    scale = int(largest_psis_loo_params[\"prior scale\"])\n",
    "    ax.plot(x, st.t.pdf(x, df=df, scale=scale))\n",
    "    ax.set_title(r'Chosen prior distribution: Student-t($\\mu$=0, $\\nu$={}, s={})'.format(df, scale));\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('best_prior.png')\n",
    "    plt.savefig('best_prior.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = fit_model(int(largest_psis_loo_params[\"prior df\"]), \n",
    "                int(largest_psis_loo_params[\"prior scale\"]))\n",
    "samples = fit.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ppc\n",
    "if not CONVERT_TO_PDF:\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    axs.hist(samples[\"p_hat_ppc\"])\n",
    "    axs.set_title(\"Posterior Probability for Surviving, \\n Expected Value from Data {:.2f}\".format(np.mean(y_test)))\n",
    "    plt.show()\n",
    "    fig.savefig(\"ppc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot betas\n",
    "if not CONVERT_TO_PDF:\n",
    "    m = 4\n",
    "    n = int(math.ceil(len(features)/float(m)))\n",
    "    fig, axs = plt.subplots(n, m, figsize=(15, 12))\n",
    "    for i, (ax, feature) in enumerate(zip(axs.flat, features)):\n",
    "        ax.hist(samples[\"beta\"][:,i], bins=100)\n",
    "        ax.set_title(feature)\n",
    "    fig.savefig('betas.png', bbox_inches='tight')\n",
    "    fig.savefig('betas.eps', bbox_inches='tight', format='eps', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = ['alpha'] + ['beta[{}]'.format(i+1) for i in range(len(features))]\n",
    "fit_logistic_regression = create_fit_table(fit, 'fit_logistic_regression.tex', filter=filt)\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    display(fit_logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    }
   ],
   "source": [
    "prior_dfs = [4, 5, 6] # redundant definition\n",
    "prior_scale = [1, 2, 4, 10, 20] # redundant definition\n",
    "model = stan_utility.compile_model('logistic_regression.stan') # redundant definition\n",
    "\n",
    "means = np.zeros((len(prior_dfs), len(prior_scale), len(features) + 1))\n",
    "intervals = np.zeros_like(means, dtype=(float, 2))\n",
    "\n",
    "if RUN_SENSITIVITY_ANALYSIS:\n",
    "    \n",
    "    for i, df in enumerate(tqdm(prior_dfs)):\n",
    "        for j, s in enumerate(prior_scale):\n",
    "            fit = fit_model(df, s, debug=False)\n",
    "            \n",
    "            # filter alpha and betas\n",
    "            filt = ['alpha'] + ['beta[{}]'.format(i+1) for i in range(len(features))]\n",
    "            filtered = create_fit_table(fit, filter=filt) \n",
    "            \n",
    "            means[i, j, :] = filtered['mean']\n",
    "            tuples = [t for t in zip(filtered['2.5%'], filtered['97.5%'])]\n",
    "            intervals[i, j, :] = tuples\n",
    "    \n",
    "    multi_index = pd.MultiIndex.from_product([prior_dfs, prior_scale], names=['prior_df', 'prior_scale'])\n",
    "\n",
    "    mean_sensitivity = pd.Panel(means).transpose(2, 0, 1).to_frame()\n",
    "    mean_sensitivity.index = multi_index\n",
    "    mean_sensitivity.columns = ['alpha'] + features\n",
    "\n",
    "    spread = np.subtract(intervals[:,:,:,1], intervals[:,:,:,0])\n",
    "    spread_sensitivity = pd.Panel(spread).transpose(2, 0, 1).to_frame()\n",
    "    spread_sensitivity.index = multi_index\n",
    "    spread_sensitivity.columns = ['alpha'] + features\n",
    "    \n",
    "    with open(\"sensitivity_mean.pkl\", 'wb') as f:\n",
    "        pickle.dump(mean_sensitivity ,f, protocol=2)\n",
    "    with open(\"sensitivity_spread.pkl\", 'wb') as f:\n",
    "        pickle.dump(spread_sensitivity ,f, protocol=2)\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RUN_SENSITIVITY_ANALYSIS:\n",
    "    with open('sensitivity_mean.pkl', 'rb') as f:\n",
    "        mean_sensitivity = pickle.load(f, encoding='latin1')\n",
    "    with open('sensitivity_spread.pkl', 'rb') as f:\n",
    "        spread_sensitivity = pickle.load(f, encoding='latin1')\n",
    "\n",
    "create_tex_table(spread_sensitivity[[f for f in features if 'embarked' not in f]], 'spread_sensitivity.tex')    \n",
    "create_tex_table(mean_sensitivity[[f for f in features if 'embarked' not in f]], 'mean_sensitivity.tex')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Performance Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x, beta, alpha):\n",
    "    return (1+np.exp(-(alpha + np.dot(x, beta))))**(-1)\n",
    "\n",
    "def get_y_preds(data, beta, alpha):\n",
    "    y_preds = []\n",
    "    for x in data:\n",
    "        res = logistic(x, beta, alpha)\n",
    "        y_preds.append(1 if res > 0.5 else 0)\n",
    "    return y_preds\n",
    "        \n",
    "def check_accuracy(data, target, beta, alpha):\n",
    "    ans_list = []\n",
    "    for i in range(len(data)):\n",
    "        res = logistic(data[i], beta, alpha)\n",
    "        ans = 1 if res > 0.5 else 0\n",
    "        ans_list.append(ans == target[i])\n",
    "\n",
    "    return np.mean(ans_list)\n",
    "\n",
    "def check_dummy_accuracy(target, res):\n",
    "    ans_list = []\n",
    "    for i in range(len(res)):\n",
    "        ans_list.append(res[i] == target[i])\n",
    "    return np.mean(ans_list)\n",
    "\n",
    "mean_list = fit.summary()[\"summary\"]\n",
    "beta = mean_list[1:len(features)+1, 0]\n",
    "alpha = mean_list[0, 0]\n",
    "\n",
    "predictive_performance = pd.DataFrame()\n",
    "predictive_performance[\"Dummy (All 1) training\"] = [f1_score(y_train, [1] * len(y_train))]\n",
    "predictive_performance[\"training\"] = [f1_score(y_train, get_y_preds(X_train, beta, alpha))]\n",
    "predictive_performance[\"Dummy (All 1) test\"] = [f1_score(y_test, [1] * len(y_test))]\n",
    "predictive_performance[\"test\"] = [f1_score(y_test, get_y_preds(X_test, beta, alpha))]\n",
    "\n",
    "create_tex_table(predictive_performance, 'f1_scores.tex')\n",
    "\n",
    "if not CONVERT_TO_PDF:\n",
    "    display(predictive_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row.names</th>\n",
       "      <th>survived</th>\n",
       "      <th>age</th>\n",
       "      <th>were_on_lifeboat</th>\n",
       "      <th>lifeboat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>row.names</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.355689</td>\n",
       "      <td>-0.389162</td>\n",
       "      <td>-0.570710</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <td>-0.355689</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.080048</td>\n",
       "      <td>0.543754</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>-0.389162</td>\n",
       "      <td>-0.080048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.138491</td>\n",
       "      <td>0.241775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>were_on_lifeboat</th>\n",
       "      <td>-0.570710</td>\n",
       "      <td>0.543754</td>\n",
       "      <td>0.138491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lifeboat</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.241775</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  row.names  survived       age  were_on_lifeboat  lifeboat\n",
       "row.names          1.000000 -0.355689 -0.389162         -0.570710  0.000000\n",
       "survived          -0.355689  1.000000 -0.080048          0.543754  0.666667\n",
       "age               -0.389162 -0.080048  1.000000          0.138491  0.241775\n",
       "were_on_lifeboat  -0.570710  0.543754  0.138491          1.000000  1.000000\n",
       "lifeboat           0.000000  0.666667  0.241775          1.000000  1.000000"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data['lifeboat'] = original_data['boat'].head().notna().astype('int')\n",
    "original_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### my_utilities.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\lstinputlisting{utilities/my_utilities.py}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic_regression.stan"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\lstinputlisting{logistic_regression.stan}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
